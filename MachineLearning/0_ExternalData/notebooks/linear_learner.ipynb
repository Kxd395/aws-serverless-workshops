{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Linear models are supervised learning algorithms used for solving either classification or regression problems.\n",
    "\n",
    "The dataset we use contains information collected from unicorn ride data. The features are measurements like health points, magic points, temperature min, temperature max, and precipitation, while the label defines if too many magic points were used for each ride. The question we have decided to answer is, \"Given the distance and weather (temp min, temp max, precipitation), were too many magic points consumed?\"\n",
    "\n",
    "We will use Amazon SageMaker's python SDK in order to train a linear learner classifier in its simplest setting. We explain uploading data to Amazon S3, training a model, and using Lambda for serverless inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Dataset\n",
    "\n",
    "We're going to work with the unicorn ride dataset we uploaded to our raw S3 bucket. After you uploaded the file, a Lambda function added a few new features: temperature min, temperature max, and precipitation amount. The Lambda function also added a boolean (zero or one) label indicating if we think too many magic points were used. Our current calculation is: magic_points >= distance * 50\n",
    "\n",
    "Provide the transformed and model bucket names found in CloudFormation. **Watch out for tabs if you copy/paste!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_bucket = '' # provide your transformed bucket name from CloudFormation\n",
    "model_bucket = '' # provide your model bucket name from CloudFormation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import boto3\n",
    "import io\n",
    "import os\n",
    "from concurrent import futures\n",
    "\n",
    "processed_prefix = 'processed'\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket(transformed_bucket)\n",
    "objects = bucket.objects.filter(Prefix=processed_prefix)\n",
    "\n",
    "folder = '/home/ec2-user/SageMaker/'\n",
    "\n",
    "def download(obj):\n",
    "    filename = os.path.join(folder,obj.key)\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    bucket.download_file(obj.key, filename)\n",
    "\n",
    "with futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    future_to_obj = {executor.submit(download, obj): obj for obj in objects}\n",
    "\n",
    "    print(\"All URLs submitted.\")\n",
    "    \n",
    "    files = 0\n",
    "    for future in futures.as_completed(future_to_obj):\n",
    "        files += 1\n",
    "    \n",
    "    print(\"All \" + str(files) + \" files downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just verify the count of how many files were downloaded at a file system level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls processed | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our files have downloaded to the notebook instance, let's load them into a dataframe.\n",
    "\n",
    "We are also going to modify the format of 'statustime' to match up with ground station data collected daily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "filenames = glob.glob(folder + \"/processed/*.csv\")\n",
    "\n",
    "list = []\n",
    "for filename in filenames:\n",
    "    list.append(pd.read_csv(filename))\n",
    "frame = pd.concat(list, axis=0, ignore_index=True)\n",
    "frame['statustime'] = pd.to_datetime(frame['statustime']).dt.to_period('D')\n",
    "display(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load our ground station weather data into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "nygs_prefix = 'nygroundstationdata'\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket(transformed_bucket)\n",
    "objects = bucket.objects.filter(Prefix=nygs_prefix)\n",
    "for obj in objects:\n",
    "    if obj.key.endswith('csv'):\n",
    "        ny = pd.read_csv(obj.get()['Body'], index_col=\"id\")\n",
    "ny['year_date'] = pd.to_datetime(ny['year_date'], format='%Y%m%d').dt.to_period('D')\n",
    "display(ny)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we have weather data across the state of New York going back to 1869. Let's pivot the table so each row has a date, weather station ID, and weather values we are interested in.\n",
    "\n",
    "Notice there are many elements in the 'ny' dataframe. We are trimming the elements to a select few. We are also replacing empty values with zero to make our future work easier.\n",
    "\n",
    "This is a key move to enable us to construct a join the easy way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_pivot = ny.pivot_table(index=['year_date','id'], columns='element', values='data_value')\n",
    "trimmed_ny_pivot = ny_pivot[['TMIN','TMAX','PRCP']].fillna(0)\n",
    "display(trimmed_ny_pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just need to merge our ride data with weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df = frame.merge(trimmed_ny_pivot, left_on=['statustime','groundstation'], right_on=['year_date','id'])\n",
    "display(merge_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing the Data\n",
    "Now that we have the raw data, let's process it. \n",
    "We'll first load the data into numpy arrays, and randomly split it into train and test with a 90/10 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "a58fdf0d-32fb-4690-add3-433cc721773d"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "processed_subdir = \"standardized\"\n",
    "train_features_file = os.path.join(folder, processed_subdir, \"train/csv/features.csv\")\n",
    "train_labels_file = os.path.join(folder, processed_subdir, \"train/csv/labels.csv\")\n",
    "test_features_file = os.path.join(folder, processed_subdir, \"test/csv/features.csv\")\n",
    "test_labels_file = os.path.join(folder, processed_subdir, \"test/csv/labels.csv\")\n",
    "\n",
    "raw = merge_df[['distance','healthpoints','magicpoints','TMIN','TMAX','PRCP','heavy_utilization']].to_numpy(dtype=np.float32)\n",
    "\n",
    "# split into train/test with a 90/10 split\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(raw)\n",
    "train_size = int(0.9 * raw.shape[0])\n",
    "train_features = raw[:train_size, :-1]\n",
    "train_labels = raw[:train_size, -1]\n",
    "test_features = raw[train_size:, :-1]\n",
    "test_labels = raw[train_size:, -1]\n",
    "\n",
    "print('train_features shape = ', train_features.shape)\n",
    "print('train_labels shape = ', train_labels.shape)\n",
    "print('test_features shape = ', test_features.shape)\n",
    "print('test_labels shape = ', test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to Amazon S3\n",
    "Now, since typically the dataset will be large and located in Amazon S3, let's write the data to Amazon S3 in recordio-protobuf format. We first create an io buffer wrapping the data, next we upload it to Amazon S3. Notice that the choice of bucket and prefix should change for different users and different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker.amazon.common as smac\n",
    "\n",
    "train_prefix = 'train'\n",
    "key = 'recordio-pb-data'\n",
    "\n",
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, train_features, train_labels)\n",
    "buf.seek(0)\n",
    "\n",
    "boto3.resource('s3').Bucket(transformed_bucket).Object(os.path.join(train_prefix, key)).upload_fileobj(buf)\n",
    "s3_train_data = 's3://{}/{}/{}'.format(transformed_bucket, train_prefix, key)\n",
    "print('uploaded training data location: {}'.format(s3_train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to provide test data. This way we can get an evaluation of the performance of the model from the training logs. In order to use this capability let's upload the test data to Amazon S3 as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prefix = 'test'\n",
    "\n",
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, test_features, test_labels)\n",
    "buf.seek(0)\n",
    "\n",
    "boto3.resource('s3').Bucket(transformed_bucket).Object(os.path.join(test_prefix, key)).upload_fileobj(buf)\n",
    "s3_test_data = 's3://{}/{}/{}'.format(transformed_bucket, test_prefix, key)\n",
    "print('uploaded test data location: {}'.format(s3_test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We take a moment to explain at a high level, how Machine Learning training and prediction works in Amazon SageMaker. First, we need to train a model. This is a process that given a labeled dataset and hyper-parameters guiding the training process,  outputs a model. Once the training is done, we set up what is called an **endpoint**. An endpoint is a web service that given a request containing an unlabeled data point, or mini-batch of data points, returns a prediction(s).\n",
    "\n",
    "In Amazon SageMaker the training is done via an object called an **estimator**. When setting up the estimator we specify the location (in Amazon S3) of the training data, the path (again in Amazon S3) to the output directory where the model will be serialized, generic hyper-parameters such as the machine type to use during the training process, and specific hyper-parameters such as the index type, etc. Once the estimator is initialized, we can call its **fit** method in order to do the actual training.\n",
    "\n",
    "Now that we are ready for training, we start with a convenience function that starts a training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "\n",
    "def trained_estimator_from_hyperparams(s3_train_data, hyperparams, output_path, s3_test_data=None):\n",
    "    \"\"\"\n",
    "    Create an Estimator from the given hyperparams, fit to training data, \n",
    "    and return a deployed predictor\n",
    "    \n",
    "    \"\"\"\n",
    "    # set up the estimator\n",
    "    linear = sagemaker.estimator.Estimator(get_image_uri(boto3.Session().region_name, \"linear-learner\"),\n",
    "        get_execution_role(),\n",
    "        train_instance_count=1,\n",
    "        train_instance_type='ml.m5.2xlarge',\n",
    "        output_path=output_path,\n",
    "        sagemaker_session=sagemaker.Session())\n",
    "    linear.set_hyperparameters(**hyperparams)\n",
    "    \n",
    "    # train a model. fit_input contains the locations of the train and test data\n",
    "    fit_input = {'train': s3_train_data}\n",
    "    if s3_test_data is not None:\n",
    "        fit_input['test'] = s3_test_data\n",
    "    linear.fit(fit_input)\n",
    "    return linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we run the actual training job. For now, we stick to default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "hyperparams = {\n",
    "    'feature_dim': int(train_features.shape[1]),\n",
    "    'mini_batch_size': int(0.1 * train_features.shape[0]),\n",
    "    'predictor_type': 'binary_classifier' \n",
    "}\n",
    "\n",
    "output_path = 's3://' + model_bucket\n",
    "linear_estimator = trained_estimator_from_hyperparams(s3_train_data, hyperparams, output_path, \n",
    "                                                   s3_test_data=s3_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we mentioned a test set in the training job. When a test set is provided the training job doesn't just produce a model but also applies it to the test set and reports the accuracy. In the logs you can view the accuracy of the model on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We now have a trained model living in S3. Instead of creating a SageMaker Endpoint, we will use Lambda to make inferences against the model.\n",
    "\n",
    "If you want to test the model using a SageMaker Endpoint before moving on, check out our documentation:\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-deploy-model.html#ex1-deploy-model-boto"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
